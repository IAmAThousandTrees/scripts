So now we've covered the basics and shown how most things can be achieved with just a small set of instructions, lets start looking to become more erudite. In human language you can often convey very precise meaning in a small number of sylables by using a greater breadth of vocabulary, especially in specialised fields, although people with less education, saddled with smaller versions of the language you speak, may not understand. But among those who do understand, not having to re-explain complex concepts every time you want to refer to them is definately a great boon. What follows is not what I consider to be the long words of assembly language though, it's just that what we have covered so far is just the baby language...

2: INTERMEDIATE LEVEL GPR INSTRUCTIONS
group 1: complex bit manipulations
bextr - extract contiguous bitfield : used to get a section from a bitfield structure
blsi - "bit, lowest set, isolate"	clears all but the lowest set bit. equivalent to: 			out = in & -in
blsmsk - "bit, lowest set, mask"	sets all bits that were the trailing zeros of the input:	out = (in - 1) & ~in
blsr - "bit, lowest set, reset"		clears the lowest set bit of the input						out = int & (in - 1)
bsf - "bit scan forward"			returns the 0-indexed bit number of the lowest value set bit
bsr - "bit scan reverse"			returns the 0-indexed bit number of the highest value set bit
bt - "bit test"						transfers the state of the 0-indexed numerically specified bit to the carry flag
btc - "bit test and complement"		"bit tests" and then changes the bit
btr - "bit test and reset"			"bit tests" and then clears the bit
bts - "bit test and set"			"bit tests" and then sets the bit
bzhi - "bits zero high"				clears all higher bits starting from the specified bit number
lzcnt - "leading zero count"		leading zeros are the ones that would be written to the left, so it's the number of zeros above the highest value set bit
tzcnt - "trailing zero count"		trailing zeros are the ones written to the right, so it's the count of zeros below the lowest value set bit.
					the result is mostly the same as the earlier "bit scan forward" but its behaviour in relation to a zero input is different and better defined
popcnt - "bit population count"		given a mask with n set bits, returns n.
pdep - "paralell bits deposit"		given a mask with n set bits, takes the lowest n bits of the input and places them in the positions of the set bits in the mask
pext - "paralell bits extract"		given a mask with n set bits, takes those bits from the input and places them in the lowest n positions of the result
shld - "shift left double"			shifts in1 left while shifting in bits from in2
shrd - "shift right double"			shifts in1 right while shifting in bits from in2

group2: threading synchronisation instructions
cmpxchg/8b/16b - "compare and exchange" - compares the destination with an earlier copy and if it is still the same swaps it with a new value. can be locked.
lock - "lock following instruction" - makes the following memory-affecting instruction atomic: in other words no other core can read or write that location until the 
										operation is finished.
pause 								tells the execution engine to stop adding looping instructions to the queue. used to preven problems caused by spin-waiting for a 
										locked section
xadd	"exchange and add"			adds B to A while keeping a copy of the original A in B. If A is a memory location, can be locked.
xchg	"exchange"					swaps B with A. if A is memory this instruction is automatically locked.


Among the bit manipulation instructions, the first four are probably the least interesting. bextr is intended for extracting a non-byte aligned or sized element from a 32 or 64-bit encoded word. Sometimes structureas are defined with minimum bit assignments in order to get the most information into the space, and this helps with reading it, though the shift-and-mask approach also works. The next three (blsi, blsmsk, blsr) help when working with bit encoded information one bit at a time, as long as it is convenient to start from the right.
Bit Scan Forward and Bit Scan Reverse start to get a bit more interesting. They both return the bit position of the end bit in a value, lowest or highest set. This can be handy if you want to then use btr to clear a bit. They don't have a fixed return when the input register is empty though, only that they will set the zero flag in that case, which can be useful in it's own way but troubling in others. This issue was 'fixed' with the introduction of more universaly standard lzcnt and tzcnt that do almost the same thing - except that lzcnt is counting from the left not from the right of the register. They both return the size of the register as the count of bits before they met a 1 if the register is empty.
popcnt is a perennially useful function, especially now k-masks are a thing.
The bit-test family is quite useful too, you can check any specific bit, change its content how you like and then respond to the prior content afterwards with anything that can be affected by a carry flag - branch, cmov, adc, rcl etc.. if targeting memory, the ones that change the bit can also be locked for thread synchronisation actions.
bzhi can be used for creating specific masks, but often filling a register with 0xffff.. and right-shifting is more appropriate.
The final four are the most interesting. pdep and pext have effects that are very hard and slow to reproduce in another way, able to pull pecified bits out of a register and turn them into a number, or throw bits into specific positions all in one go... it takes some imagination to think of the possibilities, like clearing the first 10 set bits in a 64-bit number by using the number as the mask and 0xfff...fffc00 as the value for pdep.
shld and shrd do something that's quite magic, extend the breadth of shift that you can do effectively to infinity. want to shift a block of 114MB left by 17 bits? yes you can. want to combine a set of non-byte-sized bitfields into a 64-bit word? yes you can. Again, imagination is required, these are versatile words.

When talking about thread synchronisation, I've heard some say they'd never want to do threading in assembly language. Personaly I can't see the problem: theres a few memory-operation instructions that can be locked, and locking ensures correct operation when multiple threads might be wanting to access the same memory location. You can use this to create mutexes and shared fifo's, any of the threading primatives you need. For interprocess syncronisation and thread sleeping you'll need to look into syscalls. In linux the basic one is a futex (fast userspace mutex) that you can put a bunch of threads to sleep waiting on, and wake a specific number of them, or all, from another process or within your program, but there are others.

the lock instruction is actually a prefix that acts on the next instruction, causing it to assert the memory lock signal on the 64-byte cache line containing the memory it is working on while the read-modify-write cycle is completing. many basic instructions can have their memory action made atomic, such as and, or, xor, not, add, adc, sub, sbb, inc, dec, neg. but the most useful ones are those that exchange data, or change but keep the original. these are bts, btc, btr, xchg, cmpxchg and xadd. The most used of which are xchg and xadd.

xchg is standard for implementing a lock on a critical section:

.lockloop:
	mov		eax, 1					; 1 means it is locked
	xchg	[lockaddress], eax		; we atomically swap our 1 with whatever is there
	test	eax, eax				; and then check what was there before
	jz		.wehavelock				; if it was 0 then we now have the lock
	pause							; otherwise we pause for a moment before
	jmp		.lockloop				; jumping back to try again
.wehavelock:
	; critical code section here
	; ...
	; now we must unlock:
	mov		[lockaddress], 0		; lock is only required if we don't know what is there. we know we have the lock, so we can just set it to zero.


the atomic xchg operation blocks anyone else from changing that memory between the read and the write, even if they're not using a locked instruction, so it's fine to clear the lock with a simple mov.

xadd performs an exchange but instead of just writing the value you give it back to memory, it adds the values together before writing it back. This can be used to implement a fifo work queue in just a few instructions:

.work:
	mov		qword [fifobuffer + eax * 8], 0 ; clear the fifo entry so we don't do it twice
	call	workfunction
.start_work_thread_here:
	mov		eax, 1							; amount to increment by
	lock									; make next instruction atomic
	xadd	[fifoticketaddress], eax		; get a ticket for the fifo buffer, and put the next ticket number back for the next worker
	and		eax, 0x3ff						; make the index circular: fifo is 1024 entries
.nowork:
	mov		rdi, [fifobuffer + eax * 8]		; retrieve pointer to our work from the queue
	test	rdi, rdi						; check there is work to do
	jnz		.work							; if there is work, go do it
	pause									; otherwise pause for a while
	jmp		.nowork							; then check again if some new work has come in

If there are multiple threads sourcing work then writing to the queue can be arbitrated by a similar ticketing system, or if the workers generate their own work, then an "add more work" job can be added to the end of the queue each time more work gets added, so that the first thread to get there goes to look for more work and can assume sole write access to the queue and associated data structures since there will only ever be one such job in the queue and only one thread will get the ticket that points to it.

Locked instructions take a long time to process, generally about 20 clock cycles. They should, therefore, be used sparingly.


VECTORS

A vector, in asm, is a very wide register that can be used to perform an operation to a number of data elements at once. Vector operations first appeared in x86-land about the year 2000, with the pentium MMX, which was a 32-bit cpu with some 64-bit vector instructions that allowed a small set of 64-bit vector registers to be used interchangeably as 8 bytes, 4 words, 2 dwords or 1 qword. It only had integer operations that could be performed on them, and could not be used alongside the scalar x87 floating point unit since it shared its register file. a few years later, SSE (streaming SIMD extensions - SIMD = single instruction multiple data) appeared with 128-bit registers and single-precision floating point vector instructions. Integer and double-precision floating-point operations came with SSE2, and at about that time AMD upped the GPR size to 64-bits and cemented SSE2 as part of the baseline for x86_64. more instructions and operations were added with SSE3 and SSE4 till AVX came out with 256-bit registers and a bunch more operations. then AVX2 added more, and then came FMA (fast multiply-add). Then comes AVX512 with even wider 512-bit registers and many more kinds of operations... Vectors have been a journey, to say the least, and I don't think it's over.

Vectors sound simple - get chunk of data, do simmilar things to all of data, put chunk of data back :: get things done faster. But in reality they require as much of a change in thinking about programming as great or actually much greater than the introduction of pipelining, out-of-order-execution and multithreading combined, but I will get to that later. As always we must start with the basics, and the basics with vectors are pretty basic. All the code flow and address calculation is handled by the scalar GPR operations, so at it's essence we only have to think about the data.

we move the data into and out of a vector register in the usual way, although we have to specify a vector move instruction. There are also different vector moves for integer and floating-point, since in early implementations the two had separate register files, and there was an extra delay in moving data that had been used for integer work to be used for floating-point work or vice-versa. There was also some mixup between the different register sizes and how they would treat the rest of the register in the same way as there had been in the GPRs, but thankfully they settled on the same solution by the time AVX2 came arround: clear the upper half if you only used the lower half, though SSE instructions still take the merging new lower-half data approach, so there is a basic rule: don't mix SSE with AVX. If you do there will be a very large penalty - in excess of 200 cycles - when you return to using 256-bit instructions after using SSE instructions, while all the lower-half register data gets merged with the upper-half data. There is an instruction specifically for telling the CPU to throw away the old upper-half data in order to avoid this.

There are vector equivalents of most of the GPR arithmetic and logic instructions such as add, sub, and, or, xor .. the two big differences being that nothing affects flags (which elements carry/zero/sign should be represented?) and the element size must be specified with each instruction, leading to instruction mnemonics like vpaddw (vector packed add of words) or vaddps (vector add packed single-precision-floating-point). The first treats the vector register as a set of 16-bit integers, the second as 32-bit floats. The  v at the beginning of both determines that this is the AVX version of the instruction which if it uses a 128-bit register name will clear the upper half. There is an sse version of both of these which is the same name but without the V. The other major difference between the SSE instructions and the AVX versions of the same instructions is that the AVX version specifies the 2 source registers separately from the destination register, whereas the SSE version is more like the earlier GPR instructions in specifying 2 registers, the first being changed by the second. Having the destination specified separately means that register to register moves are almost never required. You can always decide to write the result back to the first source register if that's what you want.

SSE had 8 128-bit vector registers named xmm0 to xmm7. AVX extended these to 256-bits and changed the name to ymm0 to ymm7, with the lower half still referred to as xmm0 to xmm7 like with the GPR registers being rax and eax and ax. AVX also doubled the number of registers (another reason not to use the SSE versions of instructions: SSE can still only access the first 8) to 16, so xmm8/ymm8 to xmm15/ymm15. When AVX512 came around the width doubled and the name changed to zmm, again aliasing the 256-bit and 128-bit registers as ymm and xmm, and again doubled the number of registers to 32, although again, only the AVX512 versions of instructions could access them all.

When moving data in and out of registers to memory, you also have to specify whether the data will be aligned or not. there are different move instructions for each. On older generations there was a significant slow-down when reading unaligned data from memory, so arranging for your reads to always be alígned to the size of the register if possible was an important optimisation. (alignment means that the 16, 32, or 64-byte read never crosses a cache-line boundary, important since cache-lines are the basic unit of memory-movement outside of the execution core). This optimisation doesn't seem to be important any more in recent generations of CPU, but if you try to read from an unaligned address using the aligned instruction it will still throw an exception.

Using vectors moves data much faster than GPR instructions. A vector move happens in the same time as a GPR move, but it carries more data. This means that understanding your cache limitations is more important than ever: it's quite possible for vector operations to saturate the bandwidth to/from your RAM. 

To use vectors effectively, your data has to be packed in memory in a way that looks like what you want in the vector register: a series of data elements that can all have the same processes applied to them. The typical C/C++ way of storing data in a structure is not good for vector operations, instead keep each of what would have been the structures data elements in a separate array so that 4, 8, or 16 of them can be loaded at once and 4, 8, or 16 items can be processed together, by the same series of instructions.

In the Intel sdm, being alphabetical not sectional means that there are vector instructions everywhere. There is a separate chapter for all the instructions that start with V, which are ALL vector instructions, but they are not ALL OF the vector instructions, just the new ones that have been introduced with AVX and beyond. Instructions that have been around since SSE days but have been given AVX and AVX512 versions are still listed under their original name, without the V. All the integer instructions begin with P which is convenient, but the floating-point instructions only have a common ending, either ps or pd depending on single or double precision, so they are scattered all over the place.

1: BASIC VECTOR INSTRUCTIONS
group 1: integer operations:
vmovdqa		move aligned integer data (AVX registers)
vmovdqu		move unaligned integer data (AVX registers)
vmovdqa32/64 move aligned integer data (AVX512 register, can be masked at dword or qword granularity)
vmovdqu32/64 move unaligned integer data (AVX512 registers, can be masked at dword or qword granularity)

vmovd/q		move 32/64 bits of data to/from low bits in a vector register from/to GPR or memory. if destination is vector reg, the rest of the register is cleared.
vpinsertd/q merge 32/64 bits of data into vector element (128-bit vector only) from GPR or memory. element specified by immediate.
vpextractd/q get 32/64 bits of data from vector element (128-bit vector only) to GPR or memory. element specified by immediate.

vpand			integer bitwise and (avx registers)
vpandb/w/d/q	integer bitwise and (avx512 registers , can be masked)
vpor			integer bitwise or
vporb/w/d/q		integer bitwise or (avx512, maskable)
vpxor			integer bitwise xor. vpxor reg,reg,reg can be used to clear a register to zero without using an execution port (zero idiom)
vpxorb/w/d/q	integer bitwise xor. (avx512, maskable, zero idiom if not masked)
vpaddb/w/d/q	integer add
vpsubb/w/d/q	integer subtract. (zero idiom if not masked)

before going on, and to save any more repeating myself, I will mention one other thing about avx512: avx512 introduced per-element masking (merging or clearing of results based on bitmasks in mask registers) but this required knowledge of the element size, so instructions that previously didn't care about element size like xor now require specifying an element size, even when not using masking. It also means that in effect, all instructions became 3(4)-input instructions with the third being the register being overwritten which will be merged into the result on masked-off elements. From here on I'll not make extra mention of the avx512 versions, just expect them to exist.

vpsllw/d/q	logical shift left of word, dword, qword elements by immediate value, shifting in zeros
vpsrlw/d/q	logical shift left of word, dword, qword elements by immediate value, shifting in zeros
vpsraw/d/q	arithmetic shift left of word, dword, qword elements by immediate value, maintaining sign

vpsllvw/d/q	shift, logical, left, variable. Each element is shifted by an amount held in the third input register
vpslrvw/d/q	shift, logical, right, variable. Each element is shifted by an amount held in the third input register
vpsarvw/d/q	shift, arithmetic, right, variable, preserving sign. Each element is shifted by an amount held in the third input register

vprold/q	rotate dword/qword elements left by immediate
vprord/q	rotate dword/qword elements right by immediate
	(variable rotate options provided by double-shifts, covered in advanced vector section)

vpmuldq		integer multiply of 32-bit signed values in dword 0,2,4,6,8,10,12,14 of source registers producing 64-bit results
vpmuludq	integer multiply of 32-bit unsigned values in dword 0,2,4,6,8,10,12,14 of source registers producing 64-bit results
vpmulhw		multiply 16-bit signed values and keep upper 16 bits
vpmullw		multiply 16-bit signed values and keep lower 16 bits
vpmulhrsw	multiply 16-bit values as though they are fixed point 1.15 bit values. 
				ie, multiply to 32 bits, add 0x4000 to apply rounding, then discard the top bit and bottom 15 bits.
vpmulhuw	multiply 16-bit unsigned values and keep upper 16 bits

group 2: floating-point operations:
vmovaps		move aligned packed singles
vmovapd		move aligned packed doubles
vmovups		move unaligned packed singles
vmovupd		move unaligned packed doubles
vmovss		merge single 32-bit floating point value to first element of vector. 128-bit vectors only, so upper part of register is cleared.
vmovsd		merge single 64-bit floating point value to first element of vector. 128-bit vectors only, so upper part of register is cleared.

most floating-point instructions come in 4 forms: packed singles, packed doubles, scalar singles and scalar doubles. Since direct register references are so much easier to deal with than the stack-based register format of the x87 floating-point instructions (begin with f), scalar instructions using the vector registers low bits to do a single fp operation has become normal practise for compilers, even standardising the use of xmmreg for transfer of float parameters to function calls. To avoid repeating myself, I'll not refer to them more, but know that they exist, only use 128-bit register names, and generally follow the same pattern with regards to register clearing: up to 128 bits, data other than the first element being affected is copied, above 128 bits is cleared.

vpandps/pd	bitwise and of the fp data
vporps/pd	bitwise or of the fp data
vpxorps/pd	bitwise xor of the fp data (zero idiom)
vpaddps/pd	floating-point addition
vpsubps/pd	floating-point subtraction
vpmulps/pd	floating-point multiplication
vpdivps/pd	floating-point division
vpsqrtps/pd	sqare-root operation

fma (fused multiply-add) operations multiply two values and then add a third, in a single operation that takes the same time as either done separately. All have a three-number operation sequence as part of the name. The three numbers are the 1st, 2nd and 3rd operands. the sequence indicates which two are multiplied together and which one gets added on after. The reason for specifying the sequence is to give options of which one will be overwritten (1) and which one can come from memory (3). The first two get multiplied and added to the third, so 132 denotes that the destination and the possibly memory operand will get multiplied, and then added to the other one. 231 designates that the memory operand and the other one will be multiplied and added to the destination. 213 denotes that the destination be multiplied and the memory operand added. There are also options to subtract in either direction. I'll not list all the combinations every time for clarity and brevity, just know they are there.

vfmadd132ps/pd		multiply two numbers and add to the third						A=A*C+B
vfmsub132ps/pd		multiply two numbers and subtract the third from the result		A=A*C-B
vfnmadd132ps/pd		multiply two numbers, negate result and add the third			A=B-A*C
vfnmsub132ps/pd		multiply two numbers, negate result and subtract the third		A=-A*C-B
vfmaddsub132ps/pd	multiply two numbers and then alternately add and subtract the third. elements 0,2,4,6,8,10,12,14 are the ones subtracted, the rest are added.
fvmsubadd132ps/pd	multiply two numbers and then alternately add and subtract the third. elements 1,3,5,7,9,11,13,15 are the ones subtracted, the rest are added.

there is also a selection of convertion operations to switch data types between single, double and 32-bit integer types. They are dull, except when used incorrectly. I will cover that in the advanced vector section.











